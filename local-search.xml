<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Word2Vec</title>
    <link href="/2024/05/19/Word2Vec/"/>
    <url>/2024/05/19/Word2Vec/</url>
    
    <content type="html"><![CDATA[<h3 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h3><p><strong>Word2Vec模型</strong>: word2vec是一种可以将词转换为向量表示的方法 ，但是词的向量表示只是模型训练的副产物，真正意义上的word2vec模型的训练目标是两种不同的词预测方法，对应两种不同的模型：</p><ul><li><strong>CBOW（Continuous Bag of Words）模型</strong>：基于周边词预测当前词。</li><li><strong>Skip-gram模型</strong>：基于当前词预测周边词</li></ul><p><strong>Embedding向量</strong>本质上是模型为了完成词预测任务而学习到的词语的低维表示。在训练过程中，这些向量被调整以最小化预测误差。最终，模型不仅能够完成词预测任务，还生成了有意义的词向量。</p><p>以CBOW模型举个例子，在句子”the black cat sits on the mat”中，假设上下文参数<em>window size</em>&#x3D;3时，CBOW模型根据上下文<em>context(sits)</em> &#x3D; {“the”, “black”, “cat”, “on”, “the”, “map”}预测目标词”sits”。反之，Skip-gram模型则会根据词”sits”预测其上下文*context(sits)*。这两个个例子的基本原理将会在后续部分以公式呈现。</p><h3 id="2-CBOW（Continuous-Bag-of-Words）"><a href="#2-CBOW（Continuous-Bag-of-Words）" class="headerlink" title="2. CBOW（Continuous Bag of Words）"></a>2. CBOW（Continuous Bag of Words）</h3><p>CBOW模型的目标是通过上下文词（context words）来预测中心词（target word）。即给定一组上下文词，CBOW模型尝试预测位于这些上下文词中心的目标词。</p><p>回到上述例子的内容”the black cat sits on the mat”，这里将使用不同的符号表示句子中的每个词</p><table><thead><tr><th>w1</th><th>w2</th><th>w3</th><th>w4</th><th>w5</th><th>w6</th><th>w7</th></tr></thead><tbody><tr><td>the</td><td>black</td><td>cat</td><td>sits</td><td>on</td><td>the</td><td>mat</td></tr></tbody></table><h4 id="工作原理"><a href="#工作原理" class="headerlink" title="工作原理"></a>工作原理</h4><ul><li><strong>输入</strong>：上下文词的集合（一个窗口内的词）。</li><li><strong>输出</strong>：中心词。</li></ul><p>给定一个上下文（周边词），预测中间的目标词。训练目标为求出$wt$使得<br>$$<br>p(w_t|context(w_t))<br>$$<br>条件概率最大，其中当$window_size &#x3D; 3$, $wt &#x3D; \text{sits}$时<br>$$<br>context(\text{sits}) &#x3D; {\text{the}, \text{black}, \text{cat}, \text{on}, \text{the}, \text{map}}<br>$$<br>求解时：<br>$$<br>p(w_t|context(w_t)) &#x3D; max(p(w_1|context(w_t)), \dots, p(w_7|context(w_t)))<br>$$<br>模型训练时的学习目标是最大化对数似然函数：<br>$$<br>\mathcal{L}&#x3D;\sum_{t&#x3D;1}^{T}\log p(w_{t}|context(w_t))<br>$$</p><p>CBOW模型的训练可以通过以下步骤实现：</p><ol><li>对每个上下文词进行词嵌入（embedding）。</li><li>将这些嵌入向量进行平均或求和，得到一个固定长度的向量。</li><li>使用这个向量通过一个神经网络来预测中心词。</li><li>通过反向传播更新模型参数。</li></ol><h3 id="3-Skip-gram"><a href="#3-Skip-gram" class="headerlink" title="3. Skip-gram"></a>3. Skip-gram</h3><p>Skip-gram模型的目标是通过一个中心词来预测其周围的上下文词。与CBOW相反，Skip-gram模型尝试通过单个词来预测其前后的词。</p><h4 id="工作原理-1"><a href="#工作原理-1" class="headerlink" title="工作原理"></a>工作原理</h4><ul><li><strong>输入</strong>：中心词。</li><li><strong>输出</strong>：上下文词的集合。</li></ul><p>给定一个中间词，预测周边词。求出$context(w_t)$使得<br>$$<br>p(context(w_t)|w_t)<br>$$<br>条件概率最大。</p><p>当$window_size &#x3D; 3$求解时：<br>$$<br>p(context(w_t)|w_t) &#x3D; max(p(w_{t-3}|wt)*p(w_{t-2}|wt)*p(w_{t-1}|wt)*p(w_{t+1}|wt)*p(w_{t+2}|wt)*p(w_{t+3}|wt))<br>$$</p><p>模型训练是训练目标是最大化对数似然函数：<br>$$<br>\mathcal{L}&#x3D;\sum_{t&#x3D;1}^T\sum_{-k\leq j\leq k,j\neq0}\log p(w_{t+j}\mid w_t)<br>$$<br>其中$k$为$window_size$的大小。</p><p>Skip-gram模型的训练可以通过以下步骤实现：</p><ol><li>对中心词进行词嵌入（embedding）。</li><li>使用这个嵌入向量通过一个神经网络来预测每个上下文词。</li><li>通过反向传播更新模型参数。</li></ol><h3 id="4-Architecture-and-optimization"><a href="#4-Architecture-and-optimization" class="headerlink" title="4. Architecture and optimization"></a>4. Architecture and optimization</h3><p><img src="/W2V-NET.png" alt="图片"></p><p>这里以类似的神经网络语言模型为例，上图中，$w$表示词库中的词，$C$表示词的向量表示，$tanh$为激活函数，网络的结构主要由三部分组成：</p><ul><li>输入层：输入层的输出为词向量，在CBOW模型中输出的为上下文的词向量，在Skip-gram模型中输出的为中间词的词向量。词向量作为模型参数在训练开始时会随机初始化，随着模型的训练更新</li><li>投影层：在CBOW模型中对上下文词向量进行简单求和；在Skip-gram模型中，目标词的词向量直接用于预测上下文词的概率分布。</li><li>输出层：输出层会计算词库中每一个词的概率，输入softmax函数进行归一化，得出最后的结果。</li></ul>]]></content>
    
    
    <categories>
      
      <category>NLP Basics</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>Word2Vec</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
