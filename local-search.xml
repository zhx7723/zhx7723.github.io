<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>TextClassification</title>
    <link href="/2024/08/02/TextClassification/"/>
    <url>/2024/08/02/TextClassification/</url>
    
    <content type="html"><![CDATA[<h3 id="一、文本分类简介"><a href="#一、文本分类简介" class="headerlink" title="一、文本分类简介"></a>一、文本分类简介</h3><p>在内容推荐和安全性方面，分类起到了至关重要的作用。首先，为了保证内容安全，我们需要进行多种类别的识别，例如标题党的识别、黄色反动内容的识别、低俗内容的识别以及不相关内容的识别。其次，在内容推荐系统中，我们需要对内容进行分类，例如一级类目（历史、人文、资讯、电商等）。对于商品，我们需要进行多级类目的分类，例如：蔬菜 → 根茎类蔬菜 → 萝卜（白萝卜、红萝卜、胡萝卜）。在评论挖掘方面，通过情感分析，我们可以将积极评论排在消极评论前面，提升用户体验。推荐系统在内容维度的打散和频控方面也同样依赖类别信息。因此，分类在提升推荐系统的准确性和用户体验上具有重要意义。</p><h3 id="二、不同文本分类技术介绍"><a href="#二、不同文本分类技术介绍" class="headerlink" title="二、不同文本分类技术介绍"></a>二、不同文本分类技术介绍</h3><h4 id="1-word2vec-lr"><a href="#1-word2vec-lr" class="headerlink" title="1. word2vec + lr"></a>1. word2vec + lr</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 前置文本转换</span><br>[-<span class="hljs-number">1.24389850e-01</span>, -<span class="hljs-number">2.26959527e-01</span>, -<span class="hljs-number">8.61711353e-02</span>, ... ,-<span class="hljs-number">9.74350516e-03</span>,  <span class="hljs-number">9.95748699e-01</span>]、文章所在的栏目、文章的长度、标题的长度等等   <span class="hljs-number">1</span><br>[-<span class="hljs-number">8.25783983e-02</span>, -<span class="hljs-number">6.65628314e-02</span>, -<span class="hljs-number">2.38935307e-01</span>, ... ,-<span class="hljs-number">2.12434635e-01</span>, <span class="hljs-number">5.72214313e-02</span>] 、文章所在的栏目、文章的长度、标题的长度等等   <span class="hljs-number">0</span><br></code></pre></td></tr></table></figure><p>这里需要将文本内容转换为Embedding，引入之前学习的Word Embedding内容，我们输入的序列信息是一串词的序列，为了得到文本内容的Embedding我们把这段文本所有词的Embedding相加（也可以是平均、按位取最大）作为整个句子的Embedding表示，将句子的Embedding作为模型的输入。</p><p>&#x3D;&#x3D;（思考：为什么词Embedding可以相加）&#x3D;&#x3D;</p><p>完整的操作步骤：</p><h5 id="1-jieba分词、去除停用词"><a href="#1-jieba分词、去除停用词" class="headerlink" title="1. jieba分词、去除停用词"></a>1. jieba分词、去除停用词</h5><p>读取停用词</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;data/stopwords.txt&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf8&#x27;</span>) <span class="hljs-keyword">as</span> file:<br>    line_list = file.readlines()<br>    stopword_list = [k.strip() <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> line_list]<br>    stopword_set = <span class="hljs-built_in">set</span>(stopword_list)<br></code></pre></td></tr></table></figure><p>分词+去除停用词</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">cutWords_list = []<br><span class="hljs-comment"># train_df，所有训练样本</span><br>content_series = train_df[<span class="hljs-string">&#x27;内容&#x27;</span>]<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(content_series)):<br>    <span class="hljs-comment"># 取第i行的content</span><br>    content = content_series.iloc[i]<br>    <span class="hljs-comment"># jieba.cut(content, True) True: 全局模型 | False：精确模式</span><br>    cutWords = [k <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> jieba.cut(content, <span class="hljs-literal">True</span>) <span class="hljs-keyword">if</span> k <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> stopword_set]<br>    cutWords_list.append(cutWords)<br></code></pre></td></tr></table></figure><h5 id="2-Word2Vec-模型训练"><a href="#2-Word2Vec-模型训练" class="headerlink" title="2. Word2Vec 模型训练"></a>2. Word2Vec 模型训练</h5><p>gensim.Word2Vec 参数表：</p><table><thead><tr><th>参数</th><th>取值含义</th><th>默认</th></tr></thead><tbody><tr><td>sg</td><td>0: CBOW模型 | 1: Skip-gram模型</td><td>0</td></tr><tr><td>window</td><td>int；窗口大小，window_size</td><td>5</td></tr><tr><td>size</td><td>int；词向量维度</td><td>100</td></tr><tr><td>min_count</td><td>int；最小词频，忽略出现次数较少的词</td><td>5</td></tr><tr><td>sample</td><td>float； 参数的值越小，表示下采样的强度越大，即高频词被随机忽略的概率越大。</td><td>0.001</td></tr><tr><td>hs</td><td>0: 负采样技术 | 1：分层softmax</td><td>0</td></tr><tr><td>negative</td><td>int；如果hs为0，则为负采样的个数</td><td>5</td></tr></tbody></table><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">word2vec_model = Word2Vec(cutWords_list, sg=<span class="hljs-number">1</span>, window=<span class="hljs-number">5</span>, min_count=<span class="hljs-number">10</span>, sample=<span class="hljs-number">0.001</span>, negative=<span class="hljs-number">3</span>, hs=<span class="hljs-number">1</span>, workers=<span class="hljs-number">4</span>)<br></code></pre></td></tr></table></figure><h5 id="3-Word2Vec-模型增量训练"><a href="#3-Word2Vec-模型增量训练" class="headerlink" title="3. *Word2Vec 模型增量训练"></a>3. *Word2Vec 模型增量训练</h5><p>Word2vec的一个<strong>主要局限性</strong>是它处理词汇表外（Out-of-Vocabulary, OOV）单词的能力。OOV问题指的是当遇到没有在训练语料中出现过的单词时，Word2vec模型无法为这些单词生成向量。解决的方法主要有：</p><ul><li>增量训练：在已有的Word2vec模型基础上，继续用新的数据进行训练的过程。</li><li>使用其他改进模型：fasttext模型，通过引入n-grams特征，解决了OOV的问题</li></ul><p><strong>增量训练</strong>允许模型在不重新从头开始训练的情况下，更新和调整其参数以适应新的文本数据。增量训练的主要步骤有</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">retrain_text=word2vec.Text8Corpus(<span class="hljs-string">u&#x27;./data/retrain_text2.txt&#x27;</span>) <span class="hljs-comment"># 1. 加载新词表</span><br>word2vec_model.build_vocab(retrain_text, update=<span class="hljs-literal">True</span>)          <span class="hljs-comment"># 2. 更新词汇表</span><br>word2vec_model.train(retrain_text, total_examples=<span class="hljs-number">1</span>, epochs=<span class="hljs-number">1</span>) <span class="hljs-comment"># 3. 增量训练模型</span><br></code></pre></td></tr></table></figure><p><strong>注意：</strong></p><ul><li>新词表中的词频要大于模型参数中的<em>mini_count</em> ， 否则会因为词频太低而被忽略</li><li>是实际应用中，当出现不在词表内的新词时并不会直接进行增量训练，而是暂时使用特定的标识符替换未知词汇如<unkown>，定期或是积累到一定数量时进行增量训练更新词表</li></ul><h5 id="4-LR模型训练"><a href="#4-LR模型训练" class="headerlink" title="4. LR模型训练"></a>4. LR模型训练</h5><ol><li>直接训练</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression<br><span class="hljs-comment"># 对“训练样本”进行分割，3:7,2:8</span><br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><br>train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=<span class="hljs-number">0.2</span>)<br>logisticRegression_model = LogisticRegression()<br><span class="hljs-comment"># 80%样本</span><br>logisticRegression_model.fit(train_X, train_y)<br><span class="hljs-comment"># 20%样本</span><br>logisticRegression_model.score(test_X, test_y)<br></code></pre></td></tr></table></figure><ol start="2"><li>融入shuffle 的模型训练方法</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LogisticRegression<br><span class="hljs-comment"># 重新洗牌和分裂迭代次数，为cross_val_score迭代器自定义其他交叉验证策略</span><br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> ShuffleSplit<br><span class="hljs-comment"># 交叉验证（KFold）</span><br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> cross_val_score<br><br>cv_split = ShuffleSplit(n_splits=<span class="hljs-number">5</span>, train_size=<span class="hljs-number">0.8</span>, test_size=<span class="hljs-number">0.2</span>)<br>score_ndarray = cross_val_score(LogisticRegression(), X, y, cv=cv_split)<br><span class="hljs-built_in">print</span>(score_ndarray)<br><span class="hljs-built_in">print</span>(score_ndarray.mean())<br></code></pre></td></tr></table></figure><h4 id="2-fasttext"><a href="#2-fasttext" class="headerlink" title="2. fasttext"></a>2. fasttext</h4><p>FastText扩展了 Word2vec 的概念以包括 n-gram 片段。FastText 能够有效地处理词汇表外（OOV）单词，并能够快速训练大规模语料库的词向量。</p><h5 id="关键概念"><a href="#关键概念" class="headerlink" title="关键概念"></a>关键概念</h5><p><strong>N-Grams</strong>: FastText 不仅考虑整个单词的向量表示，还将每个单词划分为一系列子词，以单词<em>where</em>和3-grams为例子，这个单词会被表示成：</p><p>$$<br>    \text{&lt;wh},\space \text{whe},\space \text{her},\space \text{ere},\space  \text{re&gt;}<br>$$</p><p>以及一个特殊序列：$\text{<where>}$</p><p><strong>注意</strong>： 这里<em>where</em>子词中的 $\text{her}$ 与 $\text{&#x2F;<her/>}$ 不等价。</p><p>最后，FastText使用单词的向量表示与所有子词的向量表示的和表示这个单词，即：</p><p>$$<br>    v_{where} &#x3D; v_{<where>} + v_{&lt;wh}+ v_{whe}+ \dots + v_{re&gt;}<br>$$</p><p>在原论文中作者在实际实现中提取了n大于或等于3且小于或等于6的所有n-grams特征， 对于单词<em>example</em>可以生成的所有$3\leq n\leq6$子词有：</p><p>$$<br>{\text{&lt;ex},\space \text{&lt;exa},\space \text{&lt;exam},\space  \text{&lt;examp},\space  \text{exa},\space  \text{exam},\space  \text{examp},\space  \text{exampl}, \dots }<br>$$</p><p>不同长度的n-grams如下表示所示：</p><table><thead><tr><th>单词</th><th>n</th><th>n-grams字符</th></tr></thead><tbody><tr><td>example</td><td>3</td><td>&lt;ex, exa, xam, amp, mpl, ple, le&gt;</td></tr><tr><td>example</td><td>4</td><td>&lt;exa, exam, xamp, ampl, mple, ple&gt;</td></tr><tr><td>example</td><td>5</td><td>&lt;exam, examp, xampl, ample, mple&gt;</td></tr><tr><td>example</td><td>6</td><td>&lt;examp, exampl, xample, ample&gt;</td></tr></tbody></table><h5 id="存储优化"><a href="#存储优化" class="headerlink" title="存储优化"></a>存储优化</h5><p>为了限制模型的内存需求，FastText论文中使用了一种哈希函数将 n-grams左右子词映射为整数值，并将这些整数限制在1到K之间。通过将每个 n-gram 子词映射到一个整数，并限制这些整数的最大值，可以大幅度减少模型对存储空间的需求。同时，每个单词仍然能通过其 n-grams 的集合保持丰富的语义信息。这种方法在处理大规模文本数据时尤其有用，因为它可以防止内存溢出并提高数据处理效率。</p><p><strong>例子</strong>：</p><ul><li><strong>单词</strong>: “hello”</li><li><strong>生成 bi-grams</strong>: ‘he’, ‘el’, ‘ll’, ‘lo’</li><li><strong>应用 FNV-1a 哈希函数</strong>:<ul><li>假设 ‘he’ 哈希后的结果是 3561</li><li>假设 ‘el’ 哈希后的结果是 1579</li><li>假设 ‘ll’ 哈希后的结果是 2086</li><li>假设 ‘lo’ 哈希后的结果是 973</li></ul></li><li><strong>词典索引</strong>: 假设 “hello” 在词典中的索引为 502</li><li><strong>单词的最终表示</strong>:<ul><li>索引: 502</li><li>哈希后的 n-grams 集合: {3561, 1579, 2086, 973}</li></ul></li></ul><p>&#x3D;&#x3D;FastText 预先为每一个可能的哈希值分配了一个向量。这些向量在模型训练过程中被学习和更新。&#x3D;&#x3D;</p><h5 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 前置文本转换</span><br>环境 不错 我 觉得 那个 虾球 什么 的 好好吃 的 感觉    __label__1<br>这家 店铺 的 虾球 不咋地 虾球 不新鲜 了               __label__0<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> fasttext<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">  训练一个监督模型, 返回一个模型对象</span><br><span class="hljs-string"></span><br><span class="hljs-string">  @param input:           训练数据文件路径</span><br><span class="hljs-string">  @param lr:              学习率</span><br><span class="hljs-string">  @param dim:             向量维度</span><br><span class="hljs-string">  @param ws:              cbow模型时使用</span><br><span class="hljs-string">  @param epoch:           次数</span><br><span class="hljs-string">  @param minCount:        词频阈值, 小于该值在初始化时会过滤掉</span><br><span class="hljs-string">  @param minCountLabel:   类别阈值，类别小于该值初始化时会过滤掉</span><br><span class="hljs-string">  @param minn:            构造subword时最小char个数</span><br><span class="hljs-string">  @param maxn:            构造subword时最大char个数</span><br><span class="hljs-string">  @param neg:             负采样</span><br><span class="hljs-string">  @param wordNgrams:      n-gram个数</span><br><span class="hljs-string">  @param loss:            损失函数类型, softmax, ns: 负采样, hs: 分层softmax</span><br><span class="hljs-string">  @param bucket:          词扩充大小, [A, B]: A语料中包含的词向量, B不在语料中的词向量</span><br><span class="hljs-string">  @param thread:          线程个数, 每个线程处理输入数据的一段, 0号线程负责loss输出</span><br><span class="hljs-string">  @param lrUpdateRate:    学习率更新</span><br><span class="hljs-string">  @param t:               负采样阈值</span><br><span class="hljs-string">  @param label:           类别前缀</span><br><span class="hljs-string">  @param pretrainedVectors: 预训练的词向量文件路径, 如果word出现在文件夹中初始化不再随机</span><br><span class="hljs-string">  @return model object</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-comment"># label的名字，和面前定义的前缀要对应上</span><br>classifier = fasttext.train_supervised(<span class="hljs-built_in">input</span>=<span class="hljs-string">&#x27;data/ft_train_df.txt&#x27;</span>, dim=<span class="hljs-number">100</span>, epoch=<span class="hljs-number">10</span>,<br>                                         lr=<span class="hljs-number">0.1</span>, wordNgrams=<span class="hljs-number">2</span>, loss=<span class="hljs-string">&#x27;softmax&#x27;</span>, label=<span class="hljs-string">&quot;__label__&quot;</span>)<br><br>classifier.save_model(<span class="hljs-string">&#x27;models/ft_classifier_1125.model&#x27;</span>)<br><br>result = classifier.test(<span class="hljs-string">&#x27;data/ft_test_df.txt&#x27;</span>)<br><span class="hljs-built_in">print</span>(result)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>NLP Basics</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>Word2Vec</tag>
      
      <tag>Fasttext</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Word2Vec</title>
    <link href="/2024/05/19/Word2Vec/"/>
    <url>/2024/05/19/Word2Vec/</url>
    
    <content type="html"><![CDATA[<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p><strong>Word2Vec模型</strong>: word2vec是一种可以将词转换为向量表示的方法 ，但是词的向量表示只是模型训练的副产物，真正意义上的word2vec模型的训练目标是两种不同的词预测方法，对应两种不同的模型：</p><ul><li><strong>CBOW（Continuous Bag of Words）模型</strong>：基于周边词预测当前词。</li><li><strong>Skip-gram模型</strong>：基于当前词预测周边词</li></ul><p><strong>Embedding向量</strong>本质上是模型为了完成词预测任务而学习到的词语的低维表示。在训练过程中，这些向量被调整以最小化预测误差。最终，模型不仅能够完成词预测任务，还生成了有意义的词向量。</p><p>以CBOW模型举个例子，在句子”the black cat sits on the mat”中，假设上下文参数<em>window size</em>&#x3D;3时，CBOW模型根据上下文<em>context(sits)</em> &#x3D; {“the”, “black”, “cat”, “on”, “the”, “map”}预测目标词”sits”。反之，Skip-gram模型则会根据词”sits”预测其上下文*context(sits)*。这两个个例子的基本原理将会在后续部分以公式呈现。</p><h2 id="2-CBOW（Continuous-Bag-of-Words）"><a href="#2-CBOW（Continuous-Bag-of-Words）" class="headerlink" title="2. CBOW（Continuous Bag of Words）"></a>2. CBOW（Continuous Bag of Words）</h2><p>CBOW模型的目标是通过上下文词（context words）来预测中心词（target word）。即给定一组上下文词，CBOW模型尝试预测位于这些上下文词中心的目标词。</p><p>回到上述例子的内容”the black cat sits on the mat”，这里将使用不同的符号表示句子中的每个词</p><table><thead><tr><th>w1</th><th>w2</th><th>w3</th><th>w4</th><th>w5</th><th>w6</th><th>w7</th></tr></thead><tbody><tr><td>the</td><td>black</td><td>cat</td><td>sits</td><td>on</td><td>the</td><td>mat</td></tr></tbody></table><h4 id="工作原理"><a href="#工作原理" class="headerlink" title="工作原理"></a>工作原理</h4><ul><li><strong>输入</strong>：上下文词的集合（一个窗口内的词）。</li><li><strong>输出</strong>：中心词。</li></ul><p>给定一个上下文（周边词），预测中间的目标词。训练目标为求出$w_t$使得<br>$$<br>p(w_t|context(w_t))<br>$$<br>条件概率最大，其中当$window{<em>}size &#x3D; 3$, $w_t &#x3D; \text{sits}$时<br>$$<br>context(\text{sits}) &#x3D; {\text{the}, \text{black}, \text{cat}, \text{on}, \text{the}, \text{map}}<br>$$<br>求解时：<br>$$<br>p(w_t|context(w_t)) &#x3D; max(p(w_1|context(w_t)), \dots, p(w_7|context(w_t)))<br>$$<br>模型训练时的学习目标是最大化对数似然函数：<br>$$<br>\mathcal{L}&#x3D;\sum</em>{t&#x3D;1}^{T}\log p(w_{t}|context(w_t))<br>$$</p><p>CBOW模型的训练可以通过以下步骤实现：</p><ol><li>对每个上下文词进行词嵌入（embedding）。</li><li>将这些嵌入向量进行平均或求和，得到一个固定长度的向量。</li><li>使用这个向量通过一个神经网络来预测中心词。</li><li>通过反向传播更新模型参数。</li></ol><h2 id="3-Skip-gram"><a href="#3-Skip-gram" class="headerlink" title="3. Skip-gram"></a>3. Skip-gram</h2><p>Skip-gram模型的目标是通过一个中心词来预测其周围的上下文词。与CBOW相反，Skip-gram模型尝试通过单个词来预测其前后的词。</p><h4 id="工作原理-1"><a href="#工作原理-1" class="headerlink" title="工作原理"></a>工作原理</h4><ul><li><strong>输入</strong>：中心词。</li><li><strong>输出</strong>：上下文词的集合。</li></ul><p>给定一个中间词，预测周边词。求出$context(w_t)$使得<br>$$<br>p(context(w_t)|w_t)<br>$$<br>条件概率最大。</p><p>当window_size 为3，求解时：<br>$$<br>p(context(w_t)|w_t) &#x3D; max(p(w_{t-3}|wt)*p(w_{t-2}|wt)*p(w_{t-1}|wt)*p(w_{t+1}|wt)*p(w_{t+2}|wt)*p(w_{t+3}|wt))<br>$$</p><p>模型训练是训练目标是最大化对数似然函数：<br>$$<br>\mathcal{L}&#x3D;\sum_{t&#x3D;1}^T\sum_{-k\leq j\leq k,j\neq0}\log p(w_{t+j}\mid w_t)<br>$$<br>其中$k$为window_size的大小。</p><p>Skip-gram模型的训练可以通过以下步骤实现：</p><ol><li>对中心词进行词嵌入（embedding）。</li><li>使用这个嵌入向量通过一个神经网络来预测每个上下文词。</li><li>通过反向传播更新模型参数。</li></ol><h2 id="4-Architecture-and-optimization"><a href="#4-Architecture-and-optimization" class="headerlink" title="4. Architecture and optimization"></a>4. Architecture and optimization</h2><p><img src="/images/W2V-NET.png"></p><p>这里以类似的神经网络语言模型为例，上图中，$w$表示词库中的词，$C$表示词的向量表示，$tanh$为激活函数，网络的结构主要由三部分组成：</p><ul><li>输入层：输入层的输出为词向量，在CBOW模型中输出的为上下文的词向量，在Skip-gram模型中输出的为中间词的词向量。词向量作为模型参数在训练开始时会随机初始化，随着模型的训练更新</li><li>投影层：在CBOW模型中对上下文词向量进行简单求和；在Skip-gram模型中，目标词的词向量直接用于预测上下文词的概率分布。</li><li>输出层：输出层会计算词库中每一个词的概率，输入softmax函数进行归一化，得出最后的结果。</li></ul><p>由于softmax需要我们遍历整个词库每个词语（类）并计算一遍输出概率并进行归一化，计算开销过大，这里使用树结构优化加速训练过程</p><h4 id="Optimization-1-分层softmax"><a href="#Optimization-1-分层softmax" class="headerlink" title="Optimization 1: 分层softmax"></a>Optimization 1: 分层softmax</h4><p>分层Softmax是一种高效的替代标准Softmax函数的方法，适用于具有大量类别输出的神经网络，尤其是在处理大词汇量的语言模型时。分层Softmax的主要作用是降低计算Softmax层在大输出空间（如大词汇量语言模型）中的计算成本。通过将输出类别（例如词汇表中的单词）组织成二叉树（哈夫曼树）的结构，它能有效减少计算复杂度。</p><p>分层Softmax的特点如下：</p><ol><li><p><strong>树结构</strong>：分层Softmax将类别组织成一棵二叉树，而不是同时考虑所有类别。树中的每个内部节点代表一个二元决策，这些决策最终指向特定的类别。</p></li><li><p><strong>路径概率</strong>：一个类别的概率是从根节点到表示该类别的叶子节点路径上所作决策概率的乘积。</p></li><li><p><strong>减少计算量</strong>：在标准Softmax中，计算每个类别的概率需要对所有类别进行归一化，而在分层Softmax中，只需要计算路径上的概率乘积，大大减少了计算量。</p></li></ol><h5 id="原理："><a href="#原理：" class="headerlink" title="原理："></a>原理：</h5><p>在分层Softmax中，假设类别树的根节点为0，类别 $w$ 的路径为 $(n_1, n_2, \ldots, n_L) $，其中每个 $ n_i $ 是路径上的节点，路径长度为 $L $。类别 $ w $ 的概率可以表示为路径上各节点决策概率的乘积：</p><p>$$<br>P(w \mid \text{context}) &#x3D; \prod_{i&#x3D;1}^{L} P(n_i \mid \text{parent}(n_i))<br>$$</p><p>其中， $P(n_i \mid \text{parent}(n_i)) $是从节点的父节点选择 $ n_i $ 的概率，通常使用Sigmoid函数来表示：</p><p>$$<br>P(n_i \mid \text{parent}(n_i)) &#x3D; \sigma(v_{n_i} \cdot h)<br>$$</p><p>其中，$v_{n_i}$ 是节点$ n_i $的向量表示，$ h $ 是上下文的隐藏层表示，$\sigma $ 是Sigmoid激活函数：</p><p>$$<br>\sigma(x) &#x3D; \frac{1}{1 + e^{-x}}<br>$$</p><h4 id="Optimization-2-负采样（Negative-Sampling）"><a href="#Optimization-2-负采样（Negative-Sampling）" class="headerlink" title="Optimization 2: 负采样（Negative Sampling）"></a>Optimization 2: 负采样（Negative Sampling）</h4><p>待续</p>]]></content>
    
    
    <categories>
      
      <category>NLP Basics</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>Word2Vec</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
